{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Measuring the pupillary light reflex with a Pupil Core headset**\n",
    "This notebook explores some functionality of the Pupil Core system and shows how, in principle, we can use it to measure the pupil's light reflex and get accurate time-critical measures (e.g. latency to constrict, time-to-peak constriction) using the World Camera to detect light onset. To accomplish this we will need to make use of the Pupil Labs Network API, which uses ZeroMQ and MessagePack for fast and reliable communication. ZeroMQ (https://zeromq.org/) is an open source universal messaging library and MessagePack (https://msgpack.org/index.html) is a binary format for computer data interchange, like JSON but faster and more efficient.\n",
    "\n",
    "To begin, make sure the tracker is plugged in and start the Pupil Capture real time application (download here: https://docs.pupil-labs.com/core/). Now we can can import zmq and and set up the Pupil Remote helper, which uses ZeroMQs REQ-REP (request-reply) pattern for reliable one-to-one communication. Pupil Remote accepts requests via a REP socket, which by default is on port 50020 (use the --port application argument to provide a custom port)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "from time import sleep, time\n",
    "import numpy as np\n",
    "import msgpack\n",
    "import pandas as pd\n",
    "import zmq\n",
    "# set up zmq context and remote helper for tracker\n",
    "context = zmq.Context()\n",
    "address = '127.0.0.1'  # remote ip or localhost\n",
    "request_port = \"50020\"  # same as in the pupil remote gui\n",
    "pupil_remote = zmq.Socket(context, zmq.REQ)\n",
    "pupil_remote.connect(\"tcp://{}:{}\".format(address, request_port))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With Pupil Remote we can issue the following basic commands:\n",
    "\n",
    "```Python\n",
    "'R' # start recording with auto generated session name\n",
    "'R rec_name' # start recording named \"rec_name\" \n",
    "'r' # stop recording\n",
    "'C' # start currently selected calibration\n",
    "'c' # stop currently selected calibration\n",
    "'T 1234.56' # resets current Pupil time to given timestamp\n",
    "'t' # get current Pupil time; returns a float as string.\n",
    "'v' # get the Pupil Core software version string\n",
    "'PUB_PORT' # return the current pub port of the IPC Backbone\n",
    "'SUB_PORT' # return the current sub port of the IPC Backbone\n",
    "```\n",
    "For example, the following code would start a recording, perform a calibration, wait for 40 seconds, and then stop the recording. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK\n",
      "OK\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "pupil_remote.send_string('R our_recording')\n",
    "print(pupil_remote.recv_string())\n",
    "\n",
    "pupil_remote.send_string('C')\n",
    "print(pupil_remote.recv_string())\n",
    "\n",
    "sleep(40)\n",
    "\n",
    "pupil_remote.send_string('r')\n",
    "print(pupil_remote.recv_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we need to receive the response for every command we send to Pupil Remote, otherwise Pupil Capture may become unresponsive. The recording just made will now be saved in the folder specified in Pupil Capture (Recorder>>Path to recordings), in a subfolder named 'our_recording'. To view the recording, just launch Pupil Player and drop in the folder. For a script demonstrating a more complete interaction with Pupil Remote, see here: https://github.com/pupil-labs/pupil-helpers/blob/master/python/pupil_remote_control.py\n",
    "\n",
    "### **Accessing data in real-time with the IPC Backbone**\n",
    "\n",
    "If we want to get real-time access to the data generated by Pupil Capture we will need to access the IPC Backbone, which is a msgpack-based API that uses ZeroMQ's PUB-SUB (publish-subscribe) pattern for one-to-many communication. The IPC Backbone is basically a message relay station that runs as a thread in the main process. To tap into the IPC Backbone we will need both the IP address and the session's unique port. These can be requested from Pupil Remote as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Request 'SUB_PORT' for reading data\n",
    "pupil_remote.send_string('SUB_PORT')\n",
    "sub_port = pupil_remote.recv_string()\n",
    "\n",
    "# Request 'PUB_PORT' for writing data\n",
    "pupil_remote.send_string('PUB_PORT')\n",
    "pub_port = pupil_remote.recv_string()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, to read data from the IPC Backbone we must *subscribe* to the *topic* we are interested in. All messages on the IPC Backbone are multipart messages which contain (at least) two message frames:\n",
    "\n",
    "- Frame 1 - contains the topic string (e.g. pupil.1.3d)\n",
    "\n",
    "- Frame 2 - contains the actual message, which is a msgpack-encoded key-value mapping. Pupil Labs uses msgpack as the serializer due to its efficient format (45% smaller than json, 200% faster than ujson) and because encoders exist for most languages\n",
    "\n",
    "Let's subscribe to the pupil data and receive a single message. Note that we need to have **msgpack** imported for serialization of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'pupil.1.3d': {b'circle_3d': {b'center': [-6.062727024239404, 7.2573373425476015, 76.67020568315203], b'normal': [-0.4862393024567709, -0.179596194083497, -0.8551704788035399], b'radius': 1.7197839210140726}, b'confidence': 0.8328064573022729, b'timestamp': 72800.4434, b'diameter_3d': 3.4395678420281453, b'ellipse': {b'center': [111.09635676517327, 178.74291486962085], b'axes': [23.059201044972536, 27.97707914059222], b'angle': 11.253623906156577}, b'location': [111.09635676517327, 178.74291486962085], b'diameter': 27.97707914059222, b'sphere': {b'center': [-0.22785539475815286, 9.412491671549565, 86.93225142879452], b'radius': 12.0}, b'projected_sphere': {b'center': [158.37493746649633, 187.1298021211465], b'axes': [171.1677743925462, 171.1677743925462], b'angle': 90.0}, b'model_confidence': 0.7356920439193004, b'model_id': 1, b'model_birth_timestamp': 72765.503835, b'theta': 1.390220371094368, b'phi': -2.0877981403098818, b'norm_pos': [0.34717611489116645, 0.2552378547099131], b'topic': b'pupil.1.3d', b'id': 1, b'method': b'3d c++'}\n"
     ]
    }
   ],
   "source": [
    "subscriber = context.socket(zmq.SUB)\n",
    "subscriber.connect(f'tcp://{address}:{sub_port}')\n",
    "subscriber.subscribe('pupil.1.3d')  # receive all pupil messages\n",
    "\n",
    "for msg in range(1):\n",
    "    topic, payload = subscriber.recv_multipart()\n",
    "    message = msgpack.loads(payload)\n",
    "    print(f\"{topic}: {message}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a more intelligible breakdown of the pupil datum that we just recieved and printed above (from https://docs.pupil-labs.com/developer/core/overview/#pupil-datum-format):\n",
    "\n",
    "```Python\n",
    "{\n",
    "    # pupil datum\n",
    "    'topic': 'pupil.0', \n",
    "    'method': '3d c++',\n",
    "    'norm_pos': [0.5, 0.5],  # norm space, [0, 1]\n",
    "    'diameter': 0.0,  # 2D image space, unit: pixel\n",
    "    'timestamp': 535741.715303987,  # time, unit: seconds\n",
    "    'confidence': 0.0,  # [0, 1]\n",
    "    \n",
    "    # 2D ellipse of the pupil in image coordinates\n",
    "    'ellipse': {  # image space, unit: pixel\n",
    "        'angle': 90.0,  # unit: degrees\n",
    "        'center': [320.0, 240.0],\n",
    "        'axes': [0.0, 0.0],\n",
    "    },\n",
    "    'id': 0,  # eye id, 0 or 1\n",
    "    \n",
    "    ## 3D model data\n",
    "    # -1 means that the model is building up and has not finished fitting\n",
    "    'model_birth_timestamp': -1.0,\n",
    "    'model_confidence': 0.0,\n",
    "    'model_id': 1,\n",
    "    \n",
    "    # pupil polar coordinates on 3D eye model. The model assumes a fixed\n",
    "    # eye ball size. Therefore there is no `radius` key\n",
    "    'theta': 0,\n",
    "    'phi': 0,\n",
    "    \n",
    "    # 3D pupil ellipse\n",
    "    'circle_3d': {  # 3D space, unit: mm\n",
    "        'normal': [0.0, -0.0, 0.0],\n",
    "        'radius': 0.0,\n",
    "        'center': [0.0, -0.0, 0.0],\n",
    "    },\n",
    "    'diameter_3d': 0.0,  # 3D space, unit: mm\n",
    "    \n",
    "    # 3D eye ball sphere\n",
    "    'sphere': {  # 3D space, unit: mm\n",
    "        'radius': 0.0,\n",
    "        'center': [0.0, -0.0, 0.0],\n",
    "    },\n",
    "    'projected_sphere': {  # image space, unit: pixel\n",
    "        'angle': 90.0,\n",
    "        'center': [0, 0],\n",
    "        'axes': [0, 0],\n",
    "    },\n",
    "}\n",
    "```\n",
    "\n",
    "The Pupil Core system generates pupil data from eye camera images. It starts by fitting an ellipse to the pupil image and then calculating the diameter of the ellipse in 2d pixel units. This measure, reported as the **diameter** variable, is two-dimensional and therefore sensitive to changes in gaze perspective (e.g. the pupil forshortening error: https://pubmed.ncbi.nlm.nih.gov/25953668/). In addition to the pupil **topic** and the **timestamp** (which get inherited from the eye image), the fields **norm_pos** and **confidence** are also added. The former is the pupil's location in normalised eye coordinates and the latter is a value between 0 and 1 indicating the quality of the measurement.\n",
    "\n",
    "By default, the Pupil Core software uses the 3d detector for pupil detection. This is an extension of the 2d detector, so its data contains keys that were inherited from the 2d detection, as well as new keys specific to the 3d model. Pupil size measurements using the 3d detection mode are valid without correction for perspective, and the best quoted error is 0.01 mm for a pupil with *r* = 5 mm. Further details on the techspecs can be found here.\n",
    "\n",
    "- https://perceptual.mpi-inf.mpg.de/files/2018/04/dierkes18_etra.pdf\n",
    "- https://arxiv.org/abs/1405.0006\n",
    "\n",
    "Section 3.2 of the first paper discusses the 3d eye model and Section 4.3 talks about pupil size estimation. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Annotations and notifications**\n",
    "To calculate time-critical measures of the pupil's light response (e.g. constriction latency, time-to-peak constriction) we will need a reliable indication in the pupil data of the time at which a light stimulus was actually administered. Pupil Labs has an **Annotation Capture** plugin that can help us with this. The plugin allows timestamps to be marked with a label, which is basically the same as sending 'messages' or 'triggers'. The labels can be created by pressing customised hotkeys in Pupil Capture or they can be created programmatically and sent to Pupil Capture via the Pupil Core Network API. If our light stimulus could be controlled programmatically and with good timing then we would be able mark the onset and offset of the light within the Pupil recording just by sending an annotation immediately before or after we issue a command to change the status of the light. To do this, we need to start the Annotation Capture plugin, either by clicking on it in Pupil Capture, or by sending a **notification**. Notifications are special messages that Pupil uses to coordinate activities. They are key-value mappings with a required 'subject' field:\n",
    "\n",
    "```Python\n",
    "notify.<notification subject>\n",
    "```\n",
    "\n",
    "For example:\n",
    "```Python\n",
    "# message topic:\n",
    "'notify.start_plugin.Annotation_Capture'\n",
    "# message payload, a notification dict\n",
    "{'subject':'recording.start_plugin', 'name':'Annotation_Capture'}\n",
    "```\n",
    "\n",
    "Notifications get passed to all active plugins via the on_notify() callback. Let's define a function that will send notifications and use it to start the Annotation Capture plugin. The same function will come in handy for general communication with Pupil Capture. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Message forwarded.'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def notify(notification):\n",
    "    \"\"\"Sends ``notification`` to Pupil Remote\"\"\"\n",
    "    topic = \"notify.\" + notification[\"subject\"]\n",
    "    payload = msgpack.dumps(notification, use_bin_type=True)\n",
    "    pupil_remote.send_string(topic, flags=zmq.SNDMORE)\n",
    "    pupil_remote.send(payload)\n",
    "    return pupil_remote.recv_string()\n",
    "\n",
    "# start the Annotation Capture plugin\n",
    "notify({\"subject\": \"start_plugin\", \"name\": \"Annotation_Capture\", \"args\": {}})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we just need functions to create and send the annotations / triggers / event markers / whatever we want to call them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def send_trigger(trigger):\n",
    "    payload = msgpack.dumps(trigger, use_bin_type=True)\n",
    "    pub_socket.send_string(trigger[\"topic\"], flags=zmq.SNDMORE)\n",
    "    pub_socket.send(payload)\n",
    "    \n",
    "def new_trigger(label, duration):\n",
    "    return {\n",
    "        \"topic\": \"annotation\",\n",
    "        \"label\": label,\n",
    "        \"timestamp\": time(),\n",
    "        \"duration\": duration\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, using the functions defined above, let's do a short recording where we send 10 messages a couple of seconds apart. Note that, before starting recording, we need to set Pupil Capture's time to the time of this script. This example is based on a more complete example here: https://github.com/pupil-labs/pupil-helpers/blob/master/python/remote_annotations.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timesync successful.\n",
      "OK\n",
      "Our event at: 1592300706.7236795\n",
      "Our event at: 1592300708.7239833\n",
      "Our event at: 1592300710.7252498\n",
      "Our event at: 1592300712.7253377\n",
      "Our event at: 1592300714.7269917\n",
      "Our event at: 1592300716.7287822\n",
      "Our event at: 1592300718.7298121\n",
      "Our event at: 1592300720.7302544\n",
      "Our event at: 1592300722.7311578\n",
      "Our event at: 1592300724.7323549\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "pub_socket = zmq.Socket(context, zmq.PUB)\n",
    "pub_socket.connect(\"tcp://{}:{}\".format(address, pub_port))\n",
    "\n",
    "# set Pupil Capture's time base to the same time as this script (should be done before starting the recording)\n",
    "pupil_remote.send_string(\"T {}\".format(time()))\n",
    "print(pupil_remote.recv_string())\n",
    "    \n",
    "pupil_remote.send_string('R our_recording')\n",
    "print(pupil_remote.recv_string())\n",
    "\n",
    "our_events = []\n",
    "for event in range(10):\n",
    "    sleep(2)\n",
    "    trigger = new_trigger(label='our_event', duration=.1)\n",
    "    send_trigger(trigger)\n",
    "    our_events.append(time())\n",
    "    print(\"Our event at: {}\".format(our_events[-1]))\n",
    "    \n",
    "sleep(2)\n",
    "\n",
    "pupil_remote.send_string('r')\n",
    "print(pupil_remote.recv_string())\n",
    "\n",
    "our_events = pd.Series(our_events)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we now find this recording and drop it into Pupil Player, making sure the **Annotation Player** plugin is active, the software will print the messages to the screen during playback. And if we export the data (press 'e' or click export button) we will have a .csv file containing our annotations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>label</th>\n",
       "      <th>duration</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>192</td>\n",
       "      <td>1.592301e+09</td>\n",
       "      <td>our_event</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>391</td>\n",
       "      <td>1.592301e+09</td>\n",
       "      <td>our_event</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>589</td>\n",
       "      <td>1.592301e+09</td>\n",
       "      <td>our_event</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>788</td>\n",
       "      <td>1.592301e+09</td>\n",
       "      <td>our_event</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>987</td>\n",
       "      <td>1.592301e+09</td>\n",
       "      <td>our_event</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1186</td>\n",
       "      <td>1.592301e+09</td>\n",
       "      <td>our_event</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1385</td>\n",
       "      <td>1.592301e+09</td>\n",
       "      <td>our_event</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1584</td>\n",
       "      <td>1.592301e+09</td>\n",
       "      <td>our_event</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1783</td>\n",
       "      <td>1.592301e+09</td>\n",
       "      <td>our_event</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1980</td>\n",
       "      <td>1.592301e+09</td>\n",
       "      <td>our_event</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index     timestamp      label  duration\n",
       "0    192  1.592301e+09  our_event       0.1\n",
       "1    391  1.592301e+09  our_event       0.1\n",
       "2    589  1.592301e+09  our_event       0.1\n",
       "3    788  1.592301e+09  our_event       0.1\n",
       "4    987  1.592301e+09  our_event       0.1\n",
       "5   1186  1.592301e+09  our_event       0.1\n",
       "6   1385  1.592301e+09  our_event       0.1\n",
       "7   1584  1.592301e+09  our_event       0.1\n",
       "8   1783  1.592301e+09  our_event       0.1\n",
       "9   1980  1.592301e+09  our_event       0.1"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "annotations = pd.read_csv(r\"C:\\Users\\engs2242\\recordings\\our_recording\\051\\exports\\000\\annotations.csv\")\n",
    "annotations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can check the timing difference between the annotation timestamps and our_events."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.000000e+00\n",
       "1    0.000000e+00\n",
       "2    0.000000e+00\n",
       "3    2.384186e-07\n",
       "4    2.384186e-07\n",
       "5    0.000000e+00\n",
       "6    2.384186e-07\n",
       "7    0.000000e+00\n",
       "8    0.000000e+00\n",
       "9    2.384186e-07\n",
       "dtype: float64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "our_events - annotations[\"timestamp\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks good inititally, but probably worth doing a more thorough test at some point."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Using the World Camera to detect light onset**\n",
    "The process outlined above looks to be reliable and efficient but it assumes no latency on the part of the actual event (e.g. a pulse of light). In our case, we can use the STLAB in asynchronous mode (i.e. with a video file) to precisely control the *duration* of a light stimulus, but we know that the command to start playing the video file will take some time to process. Usually this ranges between 100-200 ms, but it can sometimes be as high as 5 s due to various factors such as hardware, processes, whatever the LIGHT HUB is doing at the time, etc. This means that it will not suffice to simply send an annotation before /after we call `stlab.Device.play_video_file()`, as the timestamp will not be a reliable indication of light onset. Pupil constriction latencies are typically on the order of around 200-250 ms, so we will need an efficient means of time-stamping stimulus onset. One possibility is to use the WorldCam to detect the onset of the light stimulus and its associated timestamp. For this, we'll need to use the **Network APIs Frame Publisher**, making sure that the format is set to **bgr**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Message forwarded.'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#notify({\"subject\":\"start_plugin\",\"name\":\"UVC_Source\",\"args\":{\"exposure_mode\":\"manual\",\"frame_size\": (640, 480),\"frame_rate\": 60,\"name\":\"Pupil Cam1 ID2\"}})\n",
    "#notify({\"subject\":\"start_plugin\",\"name\":\"UVC_Source\",\"args\":{\"frame_size\": (640, 480),\"frame_rate\": 60,\"name\":\"Pupil Cam1 ID2\",\"exposure_mode\":\"manual\"}})\n",
    "#notify({\"subject\":\"start_plugin\", \"name\":\"Annotation_Capture\"})\n",
    "notify({\"subject\":\"frame_publishing.set_format\", \"format\":\"bgr\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Frame Publisher publishes WorldCam data in various formats under the topic frame.world. The code above starts the plugin in **bgr** format. Now we need a function that can receive the data in real time, and another function that can poll the world camera for a sudden increase in luminance and send an annotation with the associated timestamp. The ideas for these functions come from here: https://github.com/pupil-labs/pupil-helpers/blob/master/python/recv_world_video_frames.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recv_from_sub():\n",
    "    '''Recv a message with topic, payload.\n",
    "    Topic is a utf-8 encoded string. Returned as unicode object.\n",
    "    Payload is a msgpack serialized dict. Returned as a python dict.\n",
    "    Any addional message frames will be added as a list\n",
    "    in the payload dict with key: '__raw_data__' .\n",
    "    '''\n",
    "    topic = sub.recv_string()\n",
    "    payload = msgpack.unpackb(sub.recv(), raw=False)\n",
    "    extra_frames = []\n",
    "    while sub.get(zmq.RCVMORE):\n",
    "        extra_frames.append(sub.recv())\n",
    "    if extra_frames:\n",
    "        payload['__raw_data__'] = extra_frames\n",
    "    return topic, payload\n",
    "\n",
    "def detect_light_onset(trigger, threshold):\n",
    "    recent_world = None\n",
    "    recent_world_m1 = None\n",
    "    recent_world_ts = None\n",
    "    detected = False\n",
    "    print(\"Waiting for the light...\")\n",
    "    while not detected:\n",
    "        topic, msg = recv_from_sub()\n",
    "        if topic == 'frame.world':\n",
    "            recent_world = np.frombuffer(msg['__raw_data__'][0], dtype=np.uint8).reshape(msg['height'], msg['width'], 3)\n",
    "            recent_world_ts = msg['timestamp']\n",
    "        if recent_world is not None and recent_world_m1 is not None:\n",
    "            diff = recent_world.mean() - recent_world_m1.mean()\n",
    "            #print(recent_world_ts, recent_world.mean())\n",
    "            if diff > threshold:\n",
    "                print(\"Light change detected at {}\".format(recent_world_ts))\n",
    "                trigger['timestamp'] = recent_world_ts # change timestamp\n",
    "                send_trigger(trigger)\n",
    "                detected=True\n",
    "                break\n",
    "        recent_world_m1 = recent_world"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `detect_light_onset()` function will basically grab frames from the world camera (e.g. numpy arrays with shape [1280, 720, 3], depending on resolution) calculate the mean, compare it to the mean of the previous frame, and then send a timestamped message using the timestamp associated with the first frame where the difference exceeds threshold.  For this to work, **Auto Exposure Mode** for Video source in Pupil Capture must be set to **manual mode**. Let's try it out. Make sure the lighting in the room is constant and have a light source at hand (e.g. smart phone torch, light switch). It might be necessary to adjust the threshold to suit the environment / camera settings. Also, for this application, as we are only detecting a change in brightness we can use a lower resolution and take advantage of the faster frame rate that this allows (e.g. set resolution to [640, 480] and frame rate to 120). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timesync successful.\n",
      "Waiting for the light...\n",
      "Light change detected at 1592301090.6281967\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'OK'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pub_socket = zmq.Socket(context, zmq.PUB)\n",
    "pub_socket.connect(\"tcp://{}:{}\".format(address, pub_port))\n",
    "\n",
    "pupil_remote.send_string(\"T {}\".format(time()))\n",
    "print(pupil_remote.recv_string())\n",
    "\n",
    "sub = context.socket(zmq.SUB)\n",
    "sub.connect(\"tcp://{}:{}\".format(address, sub_port))\n",
    "\n",
    "# set subscriptions to topics\n",
    "# recv just pupil/gaze/notifications\n",
    "sub.setsockopt_string(zmq.SUBSCRIBE, 'frame.')\n",
    "    \n",
    "pupil_remote.send_string(\"R\")\n",
    "pupil_remote.recv_string()\n",
    "\n",
    "sleep(2.)  \n",
    "\n",
    "label = \"LIGHT_ON\"\n",
    "light_on_trigger = new_trigger(label, 1)\n",
    "detect_light_onset(light_on_trigger, 20)\n",
    "\n",
    "sleep(2)\n",
    "    \n",
    "pupil_remote.send_string(\"r\")\n",
    "pupil_remote.recv_string()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now load this recording into pupil player and make sure the Annotation Player plugin is active. Play back the video, and hopefully the LIGHT_ON annotation is stamped on the screen where the light appeared. Assuming 8.5 ms camera latency and >3 ms processing latency (as reported in the Pupil Core tech specs), this means that in theory we have timestamped the event to within 11 ms (plus up to another 1/fps). It may take some optimisation, but this principle can be used to detect the onset of a light stimulus administered by the STLAB. \n",
    "\n",
    "### **Notes on latency**\n",
    "The following notes on latency are from communications with Pupil Labs on Discord:\n",
    "\n",
    "\"*We differentiate between hardware timestamps and software timestamps. Hardware timestamps are generated by the camera at the start of the frame exposure. The software timestamps are generated by pyuvc using the system's monotonic clock at the time when the frame has finished transferring from the camera to the computer. The difference between the software and hardware timestamps is what we call camera latency. Camera latency is dependent on frame resolution, as a higher frame resolution requires more data to be transferred from the camera to the computer.*\n",
    "\n",
    "*Ideally, we would use hardware timestamps at all times. Unfortunately, we have noticed that the camera and system clocks are not necessarily synchronized at all times and on all OS. Especially on Windows, we have seen major discrepancies. This is very problematic, as every of the three cameras is using its own clock, and if they are not synchronized, pupil data cannot be matched and mapped to gaze properly.*\n",
    "\n",
    "*This is why we use corrected software timestamps instead. These are software timestamps from which we subtract a fixed amount of time to compensate the camera latency approximately.*\n",
    "\n",
    "*In summary, the recorded timestamps should correspond to the actual time at which the frame was recorded. Therefore, the relevant questions are (1) how accurate the camera latency is approximated and (2) how much it varies. (Unfortunately, I cannot give representative values for this at the time as we were not able to measure the actual camera delay on Windows due to the desynchronized clocks.) Processing latency and camera frame rate do not play a role at all in this context as they do not affect hardware nor software timestamps.*\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
